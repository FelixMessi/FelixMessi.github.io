---
layout: archive
permalink: /
title: "Hi, I'm Haokun Lin (ÊûóÊµ©Âù§) üçª"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---


I'm a Ph.D. candidate at [New Laboratory of Pattern Recognition (NLPR)](http://cripac.ia.ac.cn/en/EN/volumn/home.shtml), [Institute of Automation, Chinese Academy of Sciences](https://english.ia.cas.cn/) under the supervision of [Prof. Zhenan Sun](http://www.cbsr.ia.ac.cn/users/znsun/). I'm also a joint Ph.D. candidate at [Department of Computer Science](https://www.cs.cityu.edu.hk/), [City University of Hong Kong](https://www.cityu.edu.hk/), working with [Prof. Ying Wei](https://wei-ying.net/) and [Prof. Zhichao Lu](https://scholar.google.com/citations?user=tIFWBcQAAAAJ&hl=en). 
Before joining CASIA, I received my B.Eng. in Software Engineering from [Huazhong University of Science and Technology](https://english.hust.edu.cn/) in 2021.
 

My research interests include **<u>Multi-modal Learning</u>**, **<u>Large Language/Vision Models</u>**, and **<u>Efficient Deep Learning</u>**.

üëãüëãüëã If you're interested in my work, please feel free to reach out for discussions or collaborations!

**Contact me via**:  
üìß Mail: [haokun.lin[AT]cripac.ia.ac.cn](haokun.lin@cripac.ia.ac.cn) or [haokunlin2-c[AT]my.cityu.edu.hk](haokunlin2-c@my.cityu.edu.hk)

<h1 style="color: rgb(231, 65, 65);">üåà What's new:</h1>

<div style="height: 350px; overflow: auto; border: 1px solid #ccc; margin: 15px;">

<ul>
  <li><strong style="font-family: Consolas;">[08/2025]</strong>  üìú <b style="color: rgb(231, 165, 65);">Preprint:</b> "Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs." [<a href="https://arxiv.org/pdf/2508.14896">PDF</a>]</li>
  <li><strong style="font-family: Consolas;">[08/2025]</strong>  üìú <b style="color: rgb(231, 165, 65);">Preprint:</b> "LRQ-DiT: Log-Rotation Post-Training Quantization of Diffusion Transformers for Text-to-Image Generation." [<a href="https://www.arxiv.org/pdf/2508.03485">PDF</a>]</li>
  <li><strong style="font-family: Consolas;">[06/2025]</strong>  üéâ <b style="color: rgb(231, 165, 65);">ICCV'25:</b> "DOGR: Towards Versatile Visual Document Grounding and Referring." [<a href="https://github.com/zyinan99/DOGR">Code</a>/<a href="https://arxiv.org/pdf/2411.17125">PDF</a>]</li>
  <li><strong style="font-family: Consolas;">[05/2025]</strong>  üìú <b style="color: rgb(231, 165, 65);">Preprint:</b> "TokLIP: Marry Visual Tokens to CLIP for Multimodal Comprehension and Generation." [<a href="https://github.com/TencentARC/TokLIP">Code</a>/<a href="https://arxiv.org/pdf/2505.05422">PDF</a>]</li>
  <li><strong style="font-family: Consolas;">[02/2025]</strong>  üéâ <b style="color: rgb(231, 165, 65);">TMM:</b> "Scale Up Composed Image Retrieval Learning via Modification Text Generatio." [<a href="https://arxiv.org/pdf/2504.05316">PDF</a>]</li>
  <li><strong style="font-family: Consolas;">[01/2025]</strong>  üéâ <b style="color: rgb(231, 165, 65);">ICLR'25:</b> "Image-level Memorization Detection via Inversion-based Inference Perturbation." [<a href="https://openreview.net/pdf?id=vwOq7twk7L">PDF</a>]</li>
  <li><strong style="font-family: Consolas;">[11/2024]</strong>  üöÄ <b style="color: rgb(231, 165, 65);">Award:</b> Delighted to have received the First Prize in the 2024 Graduate Academic Forum at UCAS!</li>
  <li><strong style="font-family: Consolas;">[11/2024]</strong>  üìú <b style="color: rgb(231, 165, 65);">Preprint:</b> "DOGR: Towards Versatile Visual Document Grounding and Referring." [<a href="https://arxiv.org/pdf/2411.17125">PDF</a>]</li>
  <li><strong style="font-family: Consolas;">[11/2024]</strong>  üöÄ <b style="color: rgb(231, 165, 65);">Award:</b> Honored to be selected as a Top Reviewer at NeurIPS 2024!</li>
 <li><strong style="font-family: Consolas;">[09/2024]</strong>  üéâ <b style="color: rgb(231, 165, 65);">NeurIPS'24 Oral:</b> "DuQuant: Distributing Outliers via Dual Transformation Makes Stronger Quantized LLMs." Big Congs! üî•üî•üî• [<a href="https://github.com/Hsu1023/DuQuant">Code</a>/<a href="https://arxiv.org/pdf/2406.01721">PDF</a>]</li>
  <li><strong style="font-family: Consolas;">[07/2024]</strong>  üéâ <b style="color: rgb(231, 165, 65);">ECCV'24:</b> "MATHVERSE: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?" [<a href="https://github.com/ZrrSkywalker/MathVerse">Code</a>/<a href="https://arxiv.org/pdf/2403.14624">PDF</a>]</li>
  <li><strong style="font-family: Consolas;">[05/2024]</strong>  üéâ <b style="color: rgb(231, 165, 65);">ACL'24 Findings:</b> "IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact." [<a href="https://github.com/ruikangliu/IntactKV">Code</a>/<a href="https://arxiv.org/pdf/2403.01241">PDF</a>]</li>
  <li><strong style="font-family: Consolas;">[02/2024]</strong>  üéâ  <b style="color: rgb(231, 165, 65);">CVPR'24:</b> "MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-wise Pruning Error Metric." [<a href="https://arxiv.org/pdf/2403.07839">PDF</a>]</li>
  <li><strong style="font-family: Consolas;">[01/2024]</strong>  üéâ  <b style="color: rgb(231, 165, 65);">ICLR'24:</b> "Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models." [<a href="https://github.com/biomedical-cybernetics/Relative-importance-and-activation-pruning">Code</a>/<a href="https://openreview.net/pdf?id=Tr0lPx9woF">PDF</a>]</li>
  <li><strong style="font-family: Consolas;">[03/2022]</strong>  üéì <b style="color: rgb(231, 165, 65);">Starting Joint Ph.D.@CityU:</b> I will join Prof. <a href="https://wei-ying.net/">Ying Wei</a>'s group at CityU in 2022 Fall!</li>
  <li><strong style="font-family: Consolas;">[09/2021]</strong>  üéì <b style="color: rgb(231, 165, 65);">Starting Ph.D.@CASIA:</b> I will join Prof. <a href="http://www.cbsr.ia.ac.cn/users/znsun/">Zhenan Sun</a>'s group at NLPR, CASIA in 2021 Fall!</li>
  <li><strong style="font-family: Consolas;">[06/2021]</strong>  üéì <b style="color: rgb(231, 165, 65);">Graduation@HUST:</b> Recieved my Bachelor's Degree from Huazhong University of Science and Technology with Honorary degree.</li>
</ul>
</div>



# üéì Selected Publications ([Google Scholar](https://scholar.google.com/citations?user=7DnpUlIAAAAJ))
(\*: co-first author;  ^: corresponding author)
<table style="width:100%;border:None;border-spacing:0px;border-collapse:separate;margin-right:0;margin-left:0;margin-top:-1.5em;font-size:0.95em;">
  <tr>
    <!-- <td style="padding:8px;width:30%;vertical-align:middle;border:none;">
      <a href="images/.png">
      <img src='images/.png' width="300">
      </a>
    </td> -->
    <td style="padding:20px;width:70%;vertical-align:middle;border-right:none;border-bottom:none;">
      <b>DuQuant: Distributing Outliers via Dual Transformation Makes Stronger Quantized LLMs.</b> 
      <br>
      <u>Haokun Lin*</u>, Haobo Xu*, Yichen Wu*, Jingzhi Cui, Yingtao Zhang, Linzhan Mou, Linqi Song, Zhenan Sun^, Ying Wei^,
      <br>
      <i>in 38th Conference on Neural Information Processing Systems (<b>NeurIPS 2024 Oral</b>)</i>. 
      <br>
      [<a href="https://arxiv.org/pdf/2406.01721">PDF</a>]
      [<a href="https://arxiv.org/pdf/2406.01721">arXiv</a>]
      [<a href="https://duquant.github.io/">Project</a>]
      [<a href="https://github.com/Hsu1023/DuQuant">Github</a>]
      [<a href="https://mp.weixin.qq.com/s/lM4HeylIivW8c2o5f6J8wg">QbitAI/ÈáèÂ≠ê‰Ωç</a>] 
      <!-- [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:7ed_gRMZ2K8J:scholar.google.com/&output=citation&scisdr=ClGb7WsHEJj5ikR5kvs:AFWwaeYAAAAAZ_t_ivuNiaHr_MEN49QUocTVDlA&scisig=AFWwaeYAAAAAZ_t_isLaMkGx5aFWqySHBsqSer8&scisf=4&ct=citation&cd=-1&hl=en">bibtex</a> -->
      <!-- [<a href="https://github.com/FelixMessi/FelixMessi.github.io/blob/main/_pages/_bib/DuQuant.bib">bibtex</a>] -->
      [<a href="#" onclick="showBibDuQuant()">bibtex</a>]
      <script>
        function showBibDuQuant() {
          const bib = `@article{lin2024duquant,\n
  title={Duquant: Distributing outliers via dual transformation makes stronger quantized llms},\n
  author={Lin, Haokun and Xu, Haobo and Wu, Yichen and Cui, Jingzhi and Zhang, Yingtao and Mou, Linzhan and Song, Linqi and Sun, Zhenan and Wei, Ying},\n
  journal={Advances in Neural Information Processing Systems},\n
  volume={37},\n
  pages={87766--87800},\n
  year={2024}\n
}`;
          const newWindow = window.open("", "duquant_bibtex");
          newWindow.document.write("<pre style='font-family: monospace; padding: 20px;'>" + bib + "</pre>");
        }
      </script>
    </td>
  </tr>

  <tr>
    <!-- <td style="padding:8px;width:30%;vertical-align:middle;border:none;">
      <a href="images/.png">
      <img src='images/.png' width="300">
      </a>
    </td> -->
    <td style="padding:20px;width:70%;vertical-align:middle;border-right:none;border-bottom:none;">
       <b>MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-wise Pruning Error Metric.</b> 
      <br>
      <u>Haokun Lin</u>, Haoli Bai, Zhili Liu, Lu Hou, Muyi Sun, Linqi Song, Ying Wei^, Zhenan Sun^,
      <br>
      <i>in IEEE / CVF Computer Vision and Pattern Recognition Conference 2024 (<b>CVPR 2024</b>).</i>
      <br>
      [<a href="https://arxiv.org/pdf/2403.07839">PDF</a>]
      [<a href="https://arxiv.org/abs/2403.07839">arXiv</a>]
      <!-- [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:8JMVX1X1EywJ:scholar.google.com/&output=citation&scisdr=CgJucTIaELD3-YZ4C40:AAZF9b8AAAAAaER-E42n-faXTBB1uSdbgVA5d4s&scisig=AAZF9b8AAAAAaER-E81HajPXvjd6-r7QwOr7Mjg&scisf=4&ct=citation&cd=-1&hl=en">bibtex</a>] -->
      [<a href="#" onclick="showBibMoPE()">bibtex</a>]
      <script>
        function showBibMoPE() {
          const bib = `@inproceedings{lin2024mope,\n
  title={Mope-clip: Structured pruning for efficient vision-language models with module-wise pruning error metric},\n
  author={Lin, Haokun and Bai, Haoli and Liu, Zhili and Hou, Lu and Sun, Muyi and Song, Linqi and Wei, Ying and Sun, Zhenan},\n
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},\n
  pages={27370--27380},\n
  year={2024}\n
}`;
          const newWindow = window.open("", "mope_bibtex");
          newWindow.document.write("<pre style='font-family: monospace; padding: 20px;'>" + bib + "</pre>");
        }
      </script>
    </td>
  </tr>

  <tr>
    <!-- <td style="padding:8px;width:30%;vertical-align:middle;border:none;">
      <a href="images/.png">
      <img src='images/.png' width="300">
      </a>
    </td> -->
    <td style="padding:20px;width:70%;vertical-align:middle;border-right:none;border-bottom:none;">
       <b>TokLIP: Marry Visual Tokens to CLIP for Multimodal Comprehension and Generation.</b> 
      <br>
      <u>Haokun Lin*</u>, Teng Wang*, Yixiao Ge^, Yuying Ge, Zhichao Lu, Ying Wei, Qingfu Zhang, Zhenan Sun, Ying Shan,
      <br>
      <i>Preprint.</i>
      <br>
      [<a href="https://arxiv.org/pdf/2505.05422">PDF</a>]
      [<a href="https://arxiv.org/abs/2505.05422">arXiv</a>]
      [<a href="https://github.com/TencentARC/TokLIP">Github</a>]
      [<a href="https://huggingface.co/TencentARC/TokLIP">HuggingFace</a>]
      <!-- [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:2jh9qMaPVHgJ:scholar.google.com/&output=citation&scisdr=CgJucTIaELH0-YZ6j7Q:AAZF9b8AAAAAaER8l7RVHkiAdDbJTnz4PHg6yzY&scisig=AAZF9b8AAAAAaER8l7zmvc2_dhPOTNHwlrwvm-Y&scisf=4&ct=citation&cd=-1&hl=en">bibtex</a>] -->
      [<a href="https://mp.weixin.qq.com/s/o9nXvLmpZ9gTtGTmZsQIRQ">QbitAI/ÈáèÂ≠ê‰Ωç</a>] 
      [<a href="#" onclick="showBibTok()">bibtex</a>]
      <script>
        function showBibTok() {
          const bib = `@article{lin2025toklip,\n
  title={Toklip: Marry visual tokens to clip for multimodal comprehension and generation},\n
  author={Lin, Haokun and Wang, Teng and Ge, Yixiao and Ge, Yuying and Lu, Zhichao and Wei, Ying and Zhang, Qingfu and Sun, Zhenan and Shan, Ying},\n
  journal={arXiv preprint arXiv:2505.05422},\n
  year={2025}\n
}`;
          const newWindow = window.open("", "toklip_bibtex");
          newWindow.document.write("<pre style='font-family: monospace; padding: 20px;'>" + bib + "</pre>");
        }
      </script>
    </td>
  </tr>

  <tr>
    <!-- <td style="padding:8px;width:30%;vertical-align:middle;border:none;">
      <a href="images/.png">
      <img src='images/.png' width="300">
      </a>
    </td> -->
    <td style="padding:20px;width:70%;vertical-align:middle;border-right:none;border-bottom:none;">
       <b>Image-level Memorization Detection via Inversion-based Inference Perturbation.</b> 
      <br>
      Yue Jiang*, <u>Haokun Lin*</u>, Yang Bai, Bo Peng, Zhili Liu, Yueming Lyu, Yong Yang, Xing Zheng, Jing Dong,
      <br>
      <i>in 13th International Conference on Learning Representations (<b>ICLR 2025</b>)</i>. 
      <br>
      [<a href="https://openreview.net/pdf?id=vwOq7twk7L">PDF</a>]
      <!-- [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:QLPWAqSZxR0J:scholar.google.com/&output=citation&scisdr=ClGb7WsaEJj5ikR5wk0:AFWwaeYAAAAAZ_t_2kwQyRuZ_GFWpFMIJGCFa5g&scisig=AFWwaeYAAAAAZ_t_2tyVM3U6q28uzR1k11m-W70&scisf=4&ct=citation&cd=-1&hl=en">bibtex</a>] -->
      [<a href="#" onclick="showBibIIP()">bibtex</a>]
      <script>
        function showBibIIP() {
          const bib = `@inproceedings{jiang2025image,\n
  title={Image-level Memorization Detection via Inversion-based Inference Perturbation},\n
  author={Jiang, Yue and Lin, Haokun and Bai, Yang and Peng, Bo and Liu, Zhili and Lyu, Yueming and Yang, Yong and Dong, Jing and others},\n
  booktitle={The Thirteenth International Conference on Learning Representations},\n
  year={2025}\n
}`;
          const newWindow = window.open("", "iip_bibtex");
          newWindow.document.write("<pre style='font-family: monospace; padding: 20px;'>" + bib + "</pre>");
        }
      </script>
    </td>
  </tr>

  <tr>
    <!-- <td style="padding:8px;width:30%;vertical-align:middle;border:none;">
      <a href="images/.png">
      <img src='images/.png' width="300">
      </a>
    </td> -->
    <td style="padding:20px;width:70%;vertical-align:middle;border-right:none;border-bottom:none;">
       <b>DOGR: Towards Versatile Visual Document Grounding and Referring.</b> 
      <br>
      Yinan Zhou*, Yuxin Chen*, <u>Haokun Lin</u>, Yichen Wu, Shuyu Yang, Zhongang Qi, Chen Ma, Li Zhu, Ying Shan,
      <br>
      <i>in IEEE / CVF International Conference on Computer Vision 2025 (<b>ICCV 2025</b>)</i>. 
      <br>
      [<a href="https://arxiv.org/pdf/2411.17125">PDF</a>]
      [<a href="https://arxiv.org/abs/2411.17125">arXiv</a>]
      [<a href="https://zyinan99.github.io/">Project</a>]
      [<a href="https://github.com/zyinan99/DOGR">Github</a>]
      <!-- [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:DjTyjQjmnM0J:scholar.google.com/&output=citation&scisdr=ClGb7WsHEJj5ikSGpKM:AFWwaeYAAAAAZ_uAvKMkwoXdxdqDzpd4Arf1M7c&scisig=AFWwaeYAAAAAZ_uAvBdls6PmZoUbq32URrRMRPg&scisf=4&ct=citation&cd=-1&hl=en">bibtex</a>] -->
      [<a href="#" onclick="showBibDogr()">bibtex</a>]
      <script>
        function showBibDogr() {
          const bib = `@article{zhou2024dogr,\n
  title={DOGR: Towards versatile visual document grounding and referring},\n
  author={Zhou, Yinan and Chen, Yuxin and Lin, Haokun and Yang, Shuyu and Zhu, Li and Qi, Zhongang and Ma, Chen and Shan, Ying},\n
  journal={arXiv preprint arXiv:2411.17125},\n
  year={2024}\n
}`;
          const newWindow = window.open("", "dogr_bibtex");
          newWindow.document.write("<pre style='font-family: monospace; padding: 20px;'>" + bib + "</pre>");
        }
      </script>
    </td>
  </tr>


  <tr>
    <!-- <td style="padding:8px;width:30%;vertical-align:middle;border:none;">
      <a href="images/.png">
      <img src='images/.png' width="300">
      </a>
    </td> -->
    <td style="padding:20px;width:70%;vertical-align:middle;border-right:none;border:none;">
      <b>MATHVERSE: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?</b> 
      <br>
      Renrui Zhang*, Dongzhi Jiang*, Yichi Zhang*, <u>Haokun Lin</u>, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, Hongsheng Li,
      <br>
      <i>in 18th European Conference on Computer Vision (<b>ECCV 2024</b>)</i>. 
      <br>
      [<a href="https://arxiv.org/pdf/2403.14624">PDF</a>]
      [<a href="https://arxiv.org/abs/2403.14624">arXiv</a>]
      [<a href="https://mathverse-cuhk.github.io/">Project</a>]
      [<a href="https://github.com/ZrrSkywalker/MathVerse">Github</a>]
      [<a href="https://huggingface.co/datasets/AI4Math/MathVerse">Dataset</a>]
      [<a href="https://mp.weixin.qq.com/s/gEcCi92PdMMCItFII84lcw">Synced/Êú∫Âô®‰πãÂøÉ</a>] 
      <!-- [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:8_jFJjLGaXQJ:scholar.google.com/&output=citation&scisdr=ClGb7WsHEJj5ikSGHzU:AFWwaeYAAAAAZ_uABzWJREnGYbVlZ3AmVBpXgRc&scisig=AFWwaeYAAAAAZ_uAB6M_J38b3iYZ_1XzTjDHKGo&scisf=4&ct=citation&cd=-1&hl=en">bibtex</a>] -->
      [<a href="#" onclick="showBibmath()">bibtex</a>]
      <script>
        function showBibmath() {
          const bib = `@inproceedings{zhang2024mathverse,\n
  title={Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?},\n
  author={Zhang, Renrui and Jiang, Dongzhi and Zhang, Yichi and Lin, Haokun and Guo, Ziyu and Qiu, Pengshuo and Zhou, Aojun and Lu, Pan and Chang, Kai-Wei and Qiao, Yu and others},\n
  booktitle={European Conference on Computer Vision},\n
  pages={169--186},\n
  year={2024},\n
  organization={Springer}\n
}`;
          const newWindow = window.open("", "mathverse_bibtex");
          newWindow.document.write("<pre style='font-family: monospace; padding: 20px;'>" + bib + "</pre>");
        }
      </script>
    </td>
  </tr>

  <tr>
    <!-- <td style="padding:8px;width:30%;vertical-align:middle;border:none;">
      <a href="images/.png">
      <img src='images/.png' width="300">
      </a>
    </td> -->
    <td style="padding:20px;width:70%;vertical-align:middle;border-right:none;border-bottom:none;">
      <b>Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models.</b> 
      <br>
      Yingtao Zhang, Haoli Bai, <u>Haokun Lin</u>, Jialin Zhao, Lu Hou, Carlo Vittorio Cannistraci,
      <br>
      <i>in 12th International Conference on Learning Representations (<b>ICLR 2024</b>)</i>. 
      <br>
      [<a href="https://openreview.net/pdf?id=Tr0lPx9woF">PDF</a>]
      [<a href="https://openreview.net/forum?id=Tr0lPx9woF">OpenReview</a>]
      [<a href="https://github.com/biomedical-cybernetics/Relative-importance-and-activation-pruning">Github</a>]
      <!-- [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:fHzPozkRlIAJ:scholar.google.com/&output=citation&scisdr=ClGb7WsaEJj5ikSGRYw:AFWwaeYAAAAAZ_uAXY2CO_WRj7QUgbVq5ht6HpI&scisig=AFWwaeYAAAAAZ_uAXXT9m06SL90bosUJMTfmNTY&scisf=4&ct=citation&cd=-1&hl=en">bibtex</a>] -->
      [<a href="#" onclick="showBibRIA()">bibtex</a>]
      <script>
        function showBibRIA() {
          const bib = `@inproceedings{zhangplug,\n
  title={Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models},\n
  author={Zhang, Yingtao and Bai, Haoli and Lin, Haokun and Zhao, Jialin and Hou, Lu and Cannistraci, Carlo Vittorio},\n
  booktitle={The Twelfth International Conference on Learning Representations}\n
}`;
          const newWindow = window.open("", "plug_bibtex");
          newWindow.document.write("<pre style='font-family: monospace; padding: 20px;'>" + bib + "</pre>");
        }
      </script>
    </td>
  </tr>


<tr>
    <!-- <td style="padding:8px;width:30%;vertical-align:middle;border:none;">
      <a href="images/.png">
      <img src='images/.png' width="300">
      </a>
    </td> -->
    <td style="padding:20px;width:70%;vertical-align:middle;border-right:none;border-bottom:none;">
      <b>IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact.</b>
      <br>
      Ruikang Liu, Haoli Bai, <u>Haokun Lin</u>, Yuening Li, Han Gao, Zhengzhuo Xu, Lu Hou, Jun Yao, Chun Yuan,
      <br>
      <i>in Findings of 62nd Annual Meeting of the Association for Computational Linguistics (<b>ACL 2024 Findings</b>)</i>
      <br>
      [<a href="https://www.arxiv.org/pdf/2508.03485">PDF</a>]
      [<a href="https://www.arxiv.org/abs/2508.03485">arXiv</a>]
      [<a href="https://github.com/ruikangliu/IntactKV">Github</a>]
      <!-- [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:OkR9s_hreeMJ:scholar.google.com/&output=citation&scisdr=ClGb7WsaEJj5ikSGj3E:AFWwaeYAAAAAZ_uAl3ANRYJrWIPbZDlX9sxIqRs&scisig=AFWwaeYAAAAAZ_uAl8u3S96pRNoUs-tFPU9b4mI&scisf=4&ct=citation&cd=-1&hl=en">bibtex</a>] -->
      [<a href="#" onclick="showBibintact()">bibtex</a>]
      <script>
        function showBibintact() {
          const bib = `@article{liu2024intactkv,\n
  title={Intactkv: Improving large language model quantization by keeping pivot tokens intact},\n
  author={Liu, Ruikang and Bai, Haoli and Lin, Haokun and Li, Yuening and Gao, Han and Xu, Zhengzhuo and Hou, Lu and Yao, Jun and Yuan, Chun},\n
  journal={arXiv preprint arXiv:2403.01241},\n
  year={2024}\n
}`;
          const newWindow = window.open("", "intactkv_bibtex");
          newWindow.document.write("<pre style='font-family: monospace; padding: 20px;'>" + bib + "</pre>");
        }
      </script>
    </td>
  </tr>


  <tr>
    <!-- <td style="padding:8px;width:30%;vertical-align:middle;border:none;">
      <a href="images/.png">
      <img src='images/.png' width="300">
      </a>
    </td> -->
    <td style="padding:20px;width:70%;vertical-align:middle;border-right:none;border-bottom:none;">
       <b>Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs.</b> 
      <br>
      <u>Haokun Lin*</u>, Haobo Xu*, Yichen Wu, Ziyu Guo, Renrui Zhang, Zhichao Lu, Ying Wei, Qingfu Zhang, Zhenan Sun
      <br>
      <i>Preprint.</i>
      <br>
      [<a href="https://arxiv.org/pdf/2508.14896">PDF</a>]
      [<a href="https://arxiv.org/abs/2508.14896">arXiv</a>]
      <!-- [<a href="https://github.com/TencentARC/TokLIP">Github</a>] -->
      <!-- [<a href="https://huggingface.co/TencentARC/TokLIP">HuggingFace</a>] -->
      <!-- [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:2jh9qMaPVHgJ:scholar.google.com/&output=citation&scisdr=CgJucTIaELH0-YZ6j7Q:AAZF9b8AAAAAaER8l7RVHkiAdDbJTnz4PHg6yzY&scisig=AAZF9b8AAAAAaER8l7zmvc2_dhPOTNHwlrwvm-Y&scisf=4&ct=citation&cd=-1&hl=en">bibtex</a>] -->
      [<a href="#" onclick="showBibQDLM()">bibtex</a>]
      <script>
        function showBibQDLM() {
          const bib = `@article{lin2025quantization,\n
  title={Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs},\n
  author={Lin, Haokun and Xu, Haobo and Wu, Yichen and Guo, Ziyu and Zhang, Renrui and Lu, Zhichao and Wei, Ying and Zhang, Qingfu and Sun, Zhenan},\n
  journal={arXiv preprint arXiv:2508.14896},\n
  year={2025}\n
}`;
          const newWindow = window.open("", "qdlm_bibtex");
          newWindow.document.write("<pre style='font-family: monospace; padding: 20px;'>" + bib + "</pre>");
        }
      </script>
    </td>
  </tr>


  <tr>
    <!-- <td style="padding:8px;width:30%;vertical-align:middle;border:none;">
      <a href="images/.png">
      <img src='images/.png' width="300">
      </a>
    </td> -->
    <td style="padding:20px;width:70%;vertical-align:middle;border-right:none;border-bottom:none;">
       <b>LRQ-DiT: Log-Rotation Post-Training Quantization of Diffusion Transformers for Text-to-Image Generation.</b> 
      <br>
      Lianwei Yang*, <u>Haokun Lin*</u>, Tianchen Zhao*, Yichen Wu, Hongyu Zhu, Ruiqi Xie, Zhenan Sun, Yu Wang, Qingyi Gu,
      <br>
      <i>Preprint.</i>
      <br>
      [<a href="https://arxiv.org/pdf/2508.03485">PDF</a>]
      [<a href="https://arxiv.org/abs/2508.03485">arXiv</a>]
      <!-- [<a href="https://github.com/TencentARC/TokLIP">Github</a>]
      [<a href="https://huggingface.co/TencentARC/TokLIP">HuggingFace</a>] -->
      <!-- [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:qogzKLWPyMUJ:scholar.google.com/&output=citation&scisdr=CgJucTIPEP3LnlmuTwA:AAZF9b8AAAAAaJ2oVwCtZ_4Xy-aVbQWsehtAOBY&scisig=AAZF9b8AAAAAaJ2oV1u8g8mGP88KMEVjMC6YYpw&scisf=4&ct=citation&cd=-1&hl=en">bibtex</a>] -->
      [<a href="#" onclick="showBibLrq()">bibtex</a>]
      <script>
        function showBibLrq() {
          const bib = `@article{yang2025lrq,\n
  title={LRQ-DiT: Log-Rotation Post-Training Quantization of Diffusion Transformers for Text-to-Image Generation},\n
  author={Yang, Lianwei and Lin, Haokun and Zhao, Tianchen and Wu, Yichen and Zhu, Hongyu and Xie, Ruiqi and Sun, Zhenan and Wang, Yu and Gu, Qingyi},\n
  journal={arXiv preprint arXiv:2508.03485},\n
  year={2025}\n
}`;
          const newWindow = window.open("", "lrq_bibtex");
          newWindow.document.write("<pre style='font-family: monospace; padding: 20px;'>" + bib + "</pre>");
        }
      </script>
    </td>
  </tr>

</table>



# üèÜ Honors and Awards
- *2024.12*  First Prize, 2024 Graduate Academic Forum, University of Chinese Academy of Sciences.
- *2024.11*  NeurIPS 2024 Top Reviewer Award.
- *2021.06*  Honorary degree of HUST, Top 2%, Highest Honour for Undergraduate.
- *2020.10*  National Scholarship, P.R.China, HUST, Undergraduate Students.
- *2018-2020*  First prize, HUST Excellent Undergraduate Scholarship.
<!-- - *2016.10* Second prize, National (Senior) High School Mathematical Competition of China. -->
   

# üéñ Services
- Invited Reviewer:
  - ICDE'2025, ICML'2025, ACL'2025 Feb ARR, ICCV'2025, NeurIPS'2025.
  - EMNLP'2023, NeurIPS'2024, ICLR'2025, CVPR'2025, AISTATS'2025.
  <!-- - AIM-FM workshop@NeurIPS'2024. -->
  - IEEE Transactions on Neural Networks and Learning Systems (TNNLS).
  - Transactions on Machine Learning Research (TMLR).
- Teaching Assistant:
  - CityU, [CS1302 Introduction to Computer Programming](https://www.cityu.edu.hk/catalogue/ug/202021/course/CS1302.htm), 2025 Spring.
  - CityU, [CS5481 Data Engineering](https://www.cityu.edu.hk/catalogue/pg/202425/course/CS5481.htm), 2025 Fall.
  - CityU, [CS1302 Introduction to Computer Programming](https://www.cityu.edu.hk/catalogue/ug/202021/course/CS1302.htm), 2024 Spring.
  - CityU, [CS1315 Computer Programming](https://www.cityu.edu.hk/catalogue/ug/current/course/CS1315.htm), 2024 Fall.


# üí¨ Talks
- Invited talk at the QingKe AI about [DuQuant](https://hcqnc.xetlk.com/sl/2pnEgg).
- Invited talk at the AI Time.

--------

<center><b>Site Analytics</b></center>
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=298&t=m&d=DUrSKJKr96ryDYhhGZd-DY-6R_GeZFHFnddY0E2qqII'></script>