---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---

<!-- {% if author.googlescholar %}
  You can also find my articles on <u><a href="{{author.googlescholar}}">my Google Scholar profile</a>.</u>
{% endif %}

{% include base_path %}

{% for post in site.publications reversed %}
  {% include archive-single.html %}
{% endfor %} -->

### See full list at [Google Scholar](https://scholar.google.com/citations?user=7DnpUlIAAAAJ). ($\*$: co-first author;  ^: corresponding author)

<table style="width:100%;border:None;border-spacing:0px;border-collapse:separate;margin-right:0;margin-left:0;font-size:0.95em;">
  <tr>
    <td style="padding:5px;width:70%;vertical-align:middle;border-right:none;border-bottom:none;">
      <b>DuQuant: Distributing Outliers via Dual Transformation Makes Stronger Quantized LLMs.</b> 
      <br>
      <u>Haokun Lin*</u>, Haobo Xu*, Yichen Wu*, Jingzhi Cui, Yingtao Zhang, Linzhan Mou, Linqi Song, Zhenan Sun^, Ying Wei^,
      <br>
      <i>in 38th Conference on Neural Information Processing Systems (<b>NeurIPS 2024 Oral</b>)</i>. 
      <br>
      [<a href="https://arxiv.org/pdf/2406.01721">PDF</a>]
      [<a href="https://arxiv.org/abs/2406.01721">arXiv</a>]
      [<a href="https://duquant.github.io/">Project</a>]
      [<a href="https://github.com/Hsu1023/DuQuant">Github</a>]
      [<a href="https://mp.weixin.qq.com/s/lM4HeylIivW8c2o5f6J8wg">QbitAI/量子位</a>] 
      [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:8iXSD-jJ40cJ:scholar.google.com/&output=citation&scisdr=ClG9GyJpEN-gr-AVBmU:AFWwaeYAAAAAZ1gTHmVTCuwIqDAUyZbuC7QEvnc&scisig=AFWwaeYAAAAAZ1gTHiEDO9ff6IZDdW8xXIXGci8&scisf=4&ct=citation&cd=-1&hl=zh-CN">bibtex</a>]
    </td>
    <!-- <td style="padding:10px;width:30%;vertical-align:middle;border-right:none;border-bottom:none;">
      <a href="/images/.png">
      <img src='/images/.png' width="300">
      </a>
    </td> -->
  </tr>

  <tr>
    <td style="padding:5px;width:70%;vertical-align:middle;border-right:none;border-bottom:none;">
      <b>MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-wise Pruning Error Metric.</b>
      <br>
      <u>Haokun Lin</u>, Haoli Bai, Zhili Liu, Lu Hou, Muyi Sun, Linqi Song, Ying Wei^, Zhenan Sun^,
      <br>
      <i>in IEEE / CVF Computer Vision and Pattern Recognition Conference 2024 (<b>CVPR 2024</b>).</i>
      <br>
      [<a href="https://arxiv.org/pdf/2403.07839">PDF</a>]
      [<a href="https://arxiv.org/abs/2403.07839">arXiv</a>] 
      [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:fHzPozkRlIAJ:scholar.google.com/&output=citation&scisdr=ClG9GyJpEN-gr-AVpi8:AFWwaeYAAAAAZ1gTvi-bdGJkWBWusiXZzPn8hdY&scisig=AFWwaeYAAAAAZ1gTvlueIx_dh4dw2kkRe_TdET0&scisf=4&ct=citation&cd=-1&hl=zh-CN">bibtex</a>]
    </td>
    <!-- <td style="padding:10px;width:30%;vertical-align:middle;border-right:none;border-bottom:none;">
      <a href="/images/.png">
      <img src='/images/.png' width="300">
      </a>
    </td> -->
  </tr>

  <tr>
    <td style="padding:5px;width:70%;vertical-align:middle;border-right:none;border-bottom:none;">
      <b>Image-level Memorization Detection via Inversion-based Inference Perturbation.</b> 
      <br>
      Yue Jiang*, <u>Haokun Lin*</u>, Yang Bai, Bo Peng, Zhili Liu, Yueming Lyu, Yong Yang, Xing Zheng, Jing Dong,
      <br>
      <i>in 13th International Conference on Learning Representations (<b>ICLR 2025</b>)</i>. 
      <br>
      [<a href="https://openreview.net/pdf?id=vwOq7twk7L">PDF</a>]
      [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:QLPWAqSZxR0J:scholar.google.com/&output=citation&scisdr=ClGb7WsWEN-gr0Eirag:AFWwaeYAAAAAZ_kktahGScYdEdMt-m9-IXA1W5Q&scisig=AFWwaeYAAAAAZ_kktfNDGn7BYkcxt6KPJ9kSVmo&scisf=4&ct=citation&cd=-1&hl=zh-CN">bibtex</a>]
    </td>
    <!-- <td style="padding:10px;width:30%;vertical-align:middle;border-right:none;border-bottom:none;">
      <a href="/images/.png">
      <img src='/images/.png' width="300">
      </a>
    </td> -->
  </tr>


  <tr>
    <td style="padding:5px;width:70%;vertical-align:middle;border-right:none;border-bottom:none;">
      <b>Scale Up Composed Image Retrieval Learning via Modification Text Generation.</b> 
      <br>
      Yinan Zhou, Yaxiong Wang, <u>Haokun Lin</u>, Chen Ma, Li Zhu, Zhedong Zheng,
      <br>
      <i>in IEEE Transactions on Multimedia (<b>TMM</b>)</i>. 
      <br>
      [<a href="https://openreview.net/pdf?id=vwOq7twk7L">PDF</a>]
      [<a href="https://arxiv.org/abs/2504.05316">arXiv</a>]
      [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:oOCvJsw6D8kJ:scholar.google.com/&output=citation&scisdr=ClGb7WsQEN-gr0EgrNc:AFWwaeYAAAAAZ_kmtNfTUx-kw70UdjF2CO_dTX8&scisig=AFWwaeYAAAAAZ_kmtL72tIem_smzFok4QHYPfz8&scisf=4&ct=citation&cd=-1&hl=zh-CN">bibtex</a>]
    </td>
    <!-- <td style="padding:10px;width:30%;vertical-align:middle;border-right:none;border-bottom:none;">
      <a href="/images/.png">
      <img src='/images/.png' width="300">
      </a>
    </td> -->
  </tr>

  <tr>
    <td style="padding:5px;width:70%;vertical-align:middle;border-right:none;border-bottom:none;">
      <b>MATHVERSE: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?</b> 
      <br>
      Renrui Zhang*, Dongzhi Jiang*, Yichi Zhang*, <u>Haokun Lin</u>, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, Hongsheng Li,
      <br>
      <i>in 18th European Conference on Computer Vision (<b>ECCV 2024</b>)</i>. 
      <br>
      [<a href="https://arxiv.org/pdf/2403.14624">PDF</a>]
      [<a href="https://arxiv.org/abs/2403.14624">arXiv</a>]
      [<a href="https://mathverse-cuhk.github.io/">Project</a>]
      [<a href="https://github.com/ZrrSkywalker/MathVerse">Github</a>]
      [<a href="https://huggingface.co/datasets/AI4Math/MathVerse">Dataset</a>]
      [<a href="https://mp.weixin.qq.com/s/gEcCi92PdMMCItFII84lcw">Synced/机器之心</a>] 
      [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:OkR9s_hreeMJ:scholar.google.com/&output=citation&scisdr=ClG9GyJpEN-gr-AVxTA:AFWwaeYAAAAAZ1gT3TB5Rx3rjLOCG1M3IQaZaXI&scisig=AFWwaeYAAAAAZ1gT3ZncbIhCuf6Goq6jEIx7DwY&scisf=4&ct=citation&cd=-1&hl=zh-CN">bibtex</a>]
    </td>
    <!-- <td style="padding:10px;width:30%;vertical-align:middle;border-right:none;border-bottom:none;">
      <a href="/images/.png">
      <img src='/images/.png' width="300">
      </a>
    </td> -->
  </tr>

  <tr>
    <td style="padding:5px;width:70%;vertical-align:middle;border-right:none;border-bottom:none;">
      <b>Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models.</b> 
      <br>
      Yingtao Zhang, Haoli Bai, <u>Haokun Lin</u>, Jialin Zhao, Lu Hou, Carlo Vittorio Cannistraci,
      <br>
      <i>in 12th International Conference on Learning Representations (<b>ICLR 2024</b>)</i>. 
      <br>
      [<a href="https://openreview.net/pdf?id=Tr0lPx9woF">PDF</a>]
      [<a href="https://openreview.net/forum?id=Tr0lPx9woF">OpenReview</a>]
      [<a href="https://github.com/biomedical-cybernetics/Relative-importance-and-activation-pruning">Github</a>]
      [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:fHzPozkRlIAJ:scholar.google.com/&output=citation&scisdr=ClEMlJ8uEN-gr-8ZDsA:AFWwaeYAAAAAZ1cfFsDxbv2lWSdT7ux5BgenMfg&scisig=AFWwaeYAAAAAZ1cfFkreqZoyf_H-Wdjij5IqlDs&scisf=4&ct=citation">bibtex</a>]
    </td>
    <!-- <td style="padding:10px;width:30%;vertical-align:middle;border-right:none;border-bottom:none;">
      <a href="/images/.png">
      <img src='/images/.png' width="300">
      </a>
    </td> -->
  </tr>

  <tr>
    <td style="padding:5px;width:70%;vertical-align:middle;border-right:none;border-bottom:none;">
      <b>IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact.</b>
      <br>
      Ruikang Liu, Haoli Bai, <u>Haokun Lin</u>, Yuening Li, Han Gao, Zhengzhuo Xu, Lu Hou, Jun Yao, Chun Yuan,
      <br>
      <i>in Findings of 62nd Annual Meeting of the Association for Computational Linguistics (<b>ACL 2024 Findings</b>)</i>
      <br>
      [<a href="https://arxiv.org/pdf/2403.01241">PDF</a>]
      [<a href="https://arxiv.org/abs/2403.01241">arXiv</a>]
      [<a href="https://github.com/ruikangliu/IntactKV">Github</a>]
      [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:OkR9s_hreeMJ:scholar.google.com/&output=citation&scisdr=ClEMlJ8uEN-gr-8leYM:AFWwaeYAAAAAZ1cjYYOSP-igyi2-GTuK8YFcub8&scisig=AFWwaeYAAAAAZ1cjYV62YEauwRVwf9Y5Guz4OIw&scisf=4&ct=citation">bibtex</a>]
    </td>
    <!-- <td style="padding:10px;width:30%;vertical-align:middle;border-right:none;border-bottom:none;">
      <a href="/images/.png">
      <img src='/images/.png' width="300">
      </a>
    </td> -->
  </tr>

  <tr>
    <td style="padding:5px;width:70%;vertical-align:middle;border-right:none;border-bottom:none;">
      <b>DOGE: Towards Versatile Visual Document Grounding and Referring.</b>
      <br>
      Yinan Zhou*, Yuxin Chen*, <u>Haokun Lin</u>, Shuyu Yang, Li Zhu, Zhongang Qi, Chen Ma, Ying Shan.
      <br>
      <i>Preprint</i>
      <br>
      [<a href="https://arxiv.org/pdf/2411.17125">PDF</a>]
      [<a href="https://arxiv.org/abs/2411.17125">Arxiv</a>]
      [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:DjTyjQjmnM0J:scholar.google.com/&output=citation&scisdr=ClG9GyJpEN-gr-AV45s:AFWwaeYAAAAAZ1gT-5twrDqdtdLcEyZuvklEV1M&scisig=AFWwaeYAAAAAZ1gT-5qadQ-yOpKXQKb2Rl7XGdo&scisf=4&ct=citation&cd=-1&hl=zh-CN">bibtex</a>]
    </td>
    <!-- <td style="padding:10px;width:30%;vertical-align:middle;border-right:none;border-bottom:none;">
      <a href="/images/.png">
      <img src='/images/.png' width="300">
      </a>
    </td> -->
  </tr>
</table>