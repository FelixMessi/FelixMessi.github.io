---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---

<!-- {% if author.googlescholar %}
  You can also find my articles on <u><a href="{{author.googlescholar}}">my Google Scholar profile</a>.</u>
{% endif %}

{% include base_path %}

{% for post in site.publications reversed %}
  {% include archive-single.html %}
{% endfor %} -->

### See full list at [Google Scholar](https://scholar.google.com/citations?user=7DnpUlIAAAAJ). ($\*$: co-first author;  ^: corresponding author)

<table style="width:100%;border:None;border-spacing:0px;border-collapse:separate;margin-right:0;margin-left:0;font-size:0.95em;">
  <tr>
    <td style="padding:5px;width:70%;vertical-align:middle;border-right:none;border-bottom:none;">
      <b>DuQuant: Distributing Outliers via Dual Transformation Makes Stronger Quantized LLMs.</b> 
      <br>
      <u>Haokun Lin*</u>, Haobo Xu*, Yichen Wu*, Jingzhi Cui, Yingtao Zhang, Linzhan Mou, Linqi Song, Zhenan Sun^, Ying Wei^,
      <br>
      <i>in 38th Conference on Neural Information Processing Systems (<b>NeurIPS 2024 Oral</b>)</i>. 
      <br>
      [<a href="https://arxiv.org/pdf/2406.01721">PDF</a>]
      [<a href="https://arxiv.org/abs/2406.01721">arXiv</a>]
      [<a href="https://duquant.github.io/">Project</a>]
      [<a href="https://github.com/Hsu1023/DuQuant">Github</a>]
      [<a href="https://mp.weixin.qq.com/s/lM4HeylIivW8c2o5f6J8wg">QbitAI/量子位</a>] 
      [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:7ed_gRMZ2K8J:scholar.google.com/&output=citation&scisdr=ClGb7WsHEJj5ikR5kvs:AFWwaeYAAAAAZ_t_ivuNiaHr_MEN49QUocTVDlA&scisig=AFWwaeYAAAAAZ_t_isLaMkGx5aFWqySHBsqSer8&scisf=4&ct=citation&cd=-1&hl=en">bibtex</a>]
    </td>
    <!-- <td style="padding:10px;width:30%;vertical-align:middle;border-right:none;border-bottom:none;">
      <a href="/images/.png">
      <img src='/images/.png' width="300">
      </a>
    </td> -->
  </tr>

  <tr>
    <td style="padding:5px;width:70%;vertical-align:middle;border-right:none;border-bottom:none;">
      <b>MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-wise Pruning Error Metric.</b>
      <br>
      <u>Haokun Lin</u>, Haoli Bai, Zhili Liu, Lu Hou, Muyi Sun, Linqi Song, Ying Wei^, Zhenan Sun^,
      <br>
      <i>in IEEE / CVF Computer Vision and Pattern Recognition Conference 2024 (<b>CVPR 2024</b>).</i>
      <br>
      [<a href="https://arxiv.org/pdf/2403.07839">PDF</a>]
      [<a href="https://arxiv.org/abs/2403.07839">arXiv</a>] 
      [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:8JMVX1X1EywJ:scholar.google.com/&output=citation&scisdr=ClGb7WsHEJj5ikR5qgY:AFWwaeYAAAAAZ_t_sgZf0jXNAehvp4b2wptIGY0&scisig=AFWwaeYAAAAAZ_t_snJf8JYEvv0mKJP1MpwYz5s&scisf=4&ct=citation&cd=-1&hl=en">bibtex</a>]
    </td>
    <!-- <td style="padding:10px;width:30%;vertical-align:middle;border-right:none;border-bottom:none;">
      <a href="/images/.png">
      <img src='/images/.png' width="300">
      </a>
    </td> -->
  </tr>


  <tr>
    <td style="padding:5px;width:70%;vertical-align:middle;border-right:none;border-bottom:none;">
      <b>TokLIP: Marry Visual Tokens to CLIP for Multimodal Comprehension and Generation.
      </b> 
      <br>
      <u>Haokun Lin*</u>, Teng Wang*, Yixiao Ge^, Yuying Ge, Zhichao Lu, Ying Wei, Qingfu Zhang, Zhenan Sun, Ying Shan,
      <br>
      <i>Preprint.</i>
      <br>
      [<a href="https://arxiv.org/pdf/2505.05422">PDF</a>]
      [<a href="https://arxiv.org/abs/2505.05422">arXiv</a>]
      [<a href="https://github.com/TencentARC/TokLIP">Github</a>]
      [<a href="https://huggingface.co/TencentARC/TokLIP">HuggingFace</a>]
      [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:2jh9qMaPVHgJ:scholar.google.com/&output=citation&scisdr=CgJucTIaELH0-YZ6j7Q:AAZF9b8AAAAAaER8l7RVHkiAdDbJTnz4PHg6yzY&scisig=AAZF9b8AAAAAaER8l7zmvc2_dhPOTNHwlrwvm-Y&scisf=4&ct=citation&cd=-1&hl=en">bibtex</a>]
    </td>
    <!-- <td style="padding:10px;width:30%;vertical-align:middle;border-right:none;border-bottom:none;">
      <a href="/images/.png">
      <img src='/images/.png' width="300">
      </a>
    </td> -->
  </tr>

  <tr>
    <td style="padding:5px;width:70%;vertical-align:middle;border-right:none;border-bottom:none;">
      <b>Image-level Memorization Detection via Inversion-based Inference Perturbation.</b> 
      <br>
      Yue Jiang*, <u>Haokun Lin*</u>, Yang Bai, Bo Peng, Zhili Liu, Yueming Lyu, Yong Yang, Xing Zheng, Jing Dong,
      <br>
      <i>in 13th International Conference on Learning Representations (<b>ICLR 2025</b>)</i>. 
      <br>
      [<a href="https://openreview.net/pdf?id=vwOq7twk7L">PDF</a>]
      [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:QLPWAqSZxR0J:scholar.google.com/&output=citation&scisdr=ClGb7WsaEJj5ikR5wk0:AFWwaeYAAAAAZ_t_2kwQyRuZ_GFWpFMIJGCFa5g&scisig=AFWwaeYAAAAAZ_t_2tyVM3U6q28uzR1k11m-W70&scisf=4&ct=citation&cd=-1&hl=en">bibtex</a>]
    </td>
    <!-- <td style="padding:10px;width:30%;vertical-align:middle;border-right:none;border-bottom:none;">
      <a href="/images/.png">
      <img src='/images/.png' width="300">
      </a>
    </td> -->
  </tr>


  <tr>
    <td style="padding:5px;width:70%;vertical-align:middle;border-right:none;border-bottom:none;">
      <b>DOGR: Towards Versatile Visual Document Grounding and Referring.</b>
      <br>
      Yinan Zhou*, Yuxin Chen*, <u>Haokun Lin</u>, Yichen Wu, Shuyu Yang, Zhongang Qi, Chen Ma, Li Zhu, Ying Shan.
      <br>
      <i>in IEEE / CVF International Conference on Computer Vision 2025 (<b>ICCV 2025</b>)</i>. 
      <br>
      [<a href="https://arxiv.org/pdf/2411.17125">PDF</a>]
      [<a href="https://arxiv.org/abs/2411.17125">arXiv</a>]
      [<a href="https://zyinan99.github.io/">Project</a>]
      [<a href="https://github.com/zyinan99/DOGR">Github</a>]
      [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:DjTyjQjmnM0J:scholar.google.com/&output=citation&scisdr=ClGb7WsHEJj5ikSGpKM:AFWwaeYAAAAAZ_uAvKMkwoXdxdqDzpd4Arf1M7c&scisig=AFWwaeYAAAAAZ_uAvBdls6PmZoUbq32URrRMRPg&scisf=4&ct=citation&cd=-1&hl=en">bibtex</a>]
    </td>
    <!-- <td style="padding:10px;width:30%;vertical-align:middle;border-right:none;border-bottom:none;">
      <a href="/images/.png">
      <img src='/images/.png' width="300">
      </a>
    </td> -->
  </tr>


  <tr>
    <td style="padding:5px;width:70%;vertical-align:middle;border-right:none;border-bottom:none;">
      <b>Scale Up Composed Image Retrieval Learning via Modification Text Generation.</b> 
      <br>
      Yinan Zhou, Yaxiong Wang, <u>Haokun Lin</u>, Chen Ma, Li Zhu, Zhedong Zheng,
      <br>
      <i>in IEEE Transactions on Multimedia (<b>TMM</b>)</i>. 
      <br>
      [<a href="https://openreview.net/pdf?id=vwOq7twk7L">PDF</a>]
      [<a href="https://arxiv.org/abs/2504.05316">arXiv</a>]
      [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:oOCvJsw6D8kJ:scholar.google.com/&output=citation&scisdr=ClGb7WsHEJj5ikSGWcQ:AFWwaeYAAAAAZ_uAQcRKhx48xG71FpHQIWDwjos&scisig=AFWwaeYAAAAAZ_uAQRmnKKsrbiQtB4Fb0IJeTP4&scisf=4&ct=citation&cd=-1&hl=en">bibtex</a>]
    </td>
    <!-- <td style="padding:10px;width:30%;vertical-align:middle;border-right:none;border-bottom:none;">
      <a href="/images/.png">
      <img src='/images/.png' width="300">
      </a>
    </td> -->
  </tr>

  <tr>
    <td style="padding:5px;width:70%;vertical-align:middle;border-right:none;border-bottom:none;">
      <b>MATHVERSE: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?</b> 
      <br>
      Renrui Zhang*, Dongzhi Jiang*, Yichi Zhang*, <u>Haokun Lin</u>, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, Hongsheng Li,
      <br>
      <i>in 18th European Conference on Computer Vision (<b>ECCV 2024</b>)</i>. 
      <br>
      [<a href="https://arxiv.org/pdf/2403.14624">PDF</a>]
      [<a href="https://arxiv.org/abs/2403.14624">arXiv</a>]
      [<a href="https://mathverse-cuhk.github.io/">Project</a>]
      [<a href="https://github.com/ZrrSkywalker/MathVerse">Github</a>]
      [<a href="https://huggingface.co/datasets/AI4Math/MathVerse">Dataset</a>]
      [<a href="https://mp.weixin.qq.com/s/gEcCi92PdMMCItFII84lcw">Synced/机器之心</a>] 
      [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:8_jFJjLGaXQJ:scholar.google.com/&output=citation&scisdr=ClGb7WsHEJj5ikSGHzU:AFWwaeYAAAAAZ_uABzWJREnGYbVlZ3AmVBpXgRc&scisig=AFWwaeYAAAAAZ_uAB6M_J38b3iYZ_1XzTjDHKGo&scisf=4&ct=citation&cd=-1&hl=en">bibtex</a>]
    </td>
    <!-- <td style="padding:10px;width:30%;vertical-align:middle;border-right:none;border-bottom:none;">
      <a href="/images/.png">
      <img src='/images/.png' width="300">
      </a>
    </td> -->
  </tr>

  <tr>
    <td style="padding:5px;width:70%;vertical-align:middle;border-right:none;border-bottom:none;">
      <b>Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models.</b> 
      <br>
      Yingtao Zhang, Haoli Bai, <u>Haokun Lin</u>, Jialin Zhao, Lu Hou, Carlo Vittorio Cannistraci,
      <br>
      <i>in 12th International Conference on Learning Representations (<b>ICLR 2024</b>)</i>. 
      <br>
      [<a href="https://openreview.net/pdf?id=Tr0lPx9woF">PDF</a>]
      [<a href="https://openreview.net/forum?id=Tr0lPx9woF">OpenReview</a>]
      [<a href="https://github.com/biomedical-cybernetics/Relative-importance-and-activation-pruning">Github</a>]
      [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:fHzPozkRlIAJ:scholar.google.com/&output=citation&scisdr=ClGb7WsaEJj5ikSGRYw:AFWwaeYAAAAAZ_uAXY2CO_WRj7QUgbVq5ht6HpI&scisig=AFWwaeYAAAAAZ_uAXXT9m06SL90bosUJMTfmNTY&scisf=4&ct=citation&cd=-1&hl=en">bibtex</a>]
    </td>
    <!-- <td style="padding:10px;width:30%;vertical-align:middle;border-right:none;border-bottom:none;">
      <a href="/images/.png">
      <img src='/images/.png' width="300">
      </a>
    </td> -->
  </tr>

  <tr>
    <td style="padding:5px;width:70%;vertical-align:middle;border-right:none;border-bottom:none;">
      <b>IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact.</b>
      <br>
      Ruikang Liu, Haoli Bai, <u>Haokun Lin</u>, Yuening Li, Han Gao, Zhengzhuo Xu, Lu Hou, Jun Yao, Chun Yuan,
      <br>
      <i>in Findings of 62nd Annual Meeting of the Association for Computational Linguistics (<b>ACL 2024 Findings</b>)</i>
      <br>
      [<a href="https://arxiv.org/pdf/2403.01241">PDF</a>]
      [<a href="https://arxiv.org/abs/2403.01241">arXiv</a>]
      [<a href="https://github.com/ruikangliu/IntactKV">Github</a>]
      [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:OkR9s_hreeMJ:scholar.google.com/&output=citation&scisdr=ClGb7WsaEJj5ikSGj3E:AFWwaeYAAAAAZ_uAl3ANRYJrWIPbZDlX9sxIqRs&scisig=AFWwaeYAAAAAZ_uAl8u3S96pRNoUs-tFPU9b4mI&scisf=4&ct=citation&cd=-1&hl=en">bibtex</a>]
    </td>
    <!-- <td style="padding:10px;width:30%;vertical-align:middle;border-right:none;border-bottom:none;">
      <a href="/images/.png">
      <img src='/images/.png' width="300">
      </a>
    </td> -->
  </tr>



  <tr>
    <td style="padding:5px;width:70%;vertical-align:middle;border-right:none;border-bottom:none;">
      <b>LRQ-DiT: Log-Rotation Post-Training Quantization of Diffusion Transformers for Text-to-Image Generation.</b>
      <br>
      Lianwei Yang*, <u>Haokun Lin*</u>, Tianchen Zhao*, Yichen Wu, Hongyu Zhu, Ruiqi Xie, Zhenan Sun, Yu Wang, Qingyi Gu,
      <br>
      <i>Preprint.</i>
      <br>
      [<a href="https://arxiv.org/pdf/2505.05422">PDF</a>]
      [<a href="https://arxiv.org/abs/2505.05422">arXiv</a>]
      <!-- [<a href="https://github.com/TencentARC/TokLIP">Github</a>]
      [<a href="https://huggingface.co/TencentARC/TokLIP">HuggingFace</a>] -->
      [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:qogzKLWPyMUJ:scholar.google.com/&output=citation&scisdr=CgJucTIPEP3LnlmuTwA:AAZF9b8AAAAAaJ2oVwCtZ_4Xy-aVbQWsehtAOBY&scisig=AAZF9b8AAAAAaJ2oV1u8g8mGP88KMEVjMC6YYpw&scisf=4&ct=citation&cd=-1&hl=en">bibtex</a>]
    </td>
    <!-- <td style="padding:10px;width:30%;vertical-align:middle;border-right:none;border-bottom:none;">
      <a href="/images/.png">
      <img src='/images/.png' width="300">
      </a>
    </td> -->
  </tr>


  <tr>
    <td style="padding:5px;width:70%;vertical-align:middle;border-right:none;border-bottom:none;">
      <b>DopQ-ViT: Towards Distribution-Friendly and Outlier-Aware Post-Training Quantization for Vision Transformers.</b>
      <br>
      Lianwei Yang*, Haisong Gong*, <u>Haokun Lin*</u>, Yichen Wu, Zhenan Sun, Liang Wang, Qingyi Gu,
      <br>
      <i>Preprint.</i>
      <br>
      [<a href="https://arxiv.org/pdf/2408.03291">PDF</a>]
      [<a href="https://arxiv.org/abs/2408.03291">arXiv</a>]
      <!-- [<a href="https://github.com/TencentARC/TokLIP">Github</a>]
      [<a href="https://huggingface.co/TencentARC/TokLIP">HuggingFace</a>] -->
      [<a href="https://scholar.googleusercontent.com/scholar.bib?q=info:hjUx9US2ziQJ:scholar.google.com/&output=citation&scisdr=CgJucTIeEJXbnlmp4Q0:AAZF9b8AAAAAaJ2v-Q3QOHYGHbwkSzLuhRjzSI0&scisig=AAZF9b8AAAAAaJ2v-YZJFXXmQ57IvKwZPmQF72E&scisf=4&ct=citation&cd=-1&hl=en">bibtex</a>]
    </td>
    <!-- <td style="padding:10px;width:30%;vertical-align:middle;border-right:none;border-bottom:none;">
      <a href="/images/.png">
      <img src='/images/.png' width="300">
      </a>
    </td> -->
  </tr>

</table>